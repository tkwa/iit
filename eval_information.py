from torch import nn
from iit.utils.wrapper import get_hook_points
from iit.model_pairs import IITProbeSequentialPair
from iit.utils.probes import train_probes_on_model_pair, evaluate_probe
from iit.tasks.task_loader import get_alignment, get_dataset
import torch as t
from tqdm import tqdm
from iit.utils.plotter import plot_probe_stats
import os
import wandb
from datetime import datetime

def evaluate_model_on_probes(ll_model: t.nn.Module, task: str, probe_training_args: dict, 
                             train_set: t.utils.data.Dataset, test_set: t.utils.data.Dataset,
                             use_wandb: bool = False, verbose: bool = False, save_probes: bool = False):
    print("reached evaluate_model!")
    probe_stats_per_layer = {}
    log_stats_per_layer = {}
    if use_wandb:
        wandb.init(project="iit")
        wandb.run.name = f"{task}_probes"
        wandb.run.save()
        # add training args to wandb config
        wandb.config.update(probe_training_args)

    for hook_point in tqdm(get_hook_points(ll_model), desc="Hook points"):
        _, hl_model, corr = get_alignment(task, config={
            'hook_point': hook_point,
            "input_shape": test_set.get_input_shape()
        })
        model_pair = IITProbeSequentialPair(
            ll_model=ll_model, hl_model=hl_model, corr=corr, training_args=probe_training_args)
        
        input_shape = train_set.get_input_shape()
        trainer_out = train_probes_on_model_pair(model_pair, input_shape, train_set, probe_training_args)
        # get everything but probes from trainer_out
        log_stats_per_layer[hook_point] = {k: v for k, v in trainer_out.items() if k != 'probes'}
        probe_stats_per_layer[hook_point] = trainer_out
        # save probe model
        if save_probes:
            for k, v in trainer_out['probes'].items():
                if not os.path.exists(f"weights/probes/{task}/{hook_point}"):
                    os.makedirs(f"weights/probes/{task}/{hook_point}")
                t.save(v, f"weights/probes/{task}/{hook_point}/{k}.pt")

        if use_wandb:
            wandb.log({'accuracy': trainer_out['accuracy']})
            wandb.log({'loss': trainer_out['loss']})

        # find test accuracy
        evals_out = evaluate_probe(trainer_out['probes'], model_pair, test_set, nn.CrossEntropyLoss())
        if verbose:
            print(f"hook_point: {hook_point}")
            print(f"accuracy: {trainer_out['accuracy']}")
            print(f"loss: {trainer_out['loss']}")
            print(f"evals_out: {evals_out}")
        probe_stats_per_layer[hook_point].update(evals_out)
        if use_wandb:
            wandb.log(log_stats_per_layer[hook_point])

    if verbose:
        print(f"probe_stats_per_layer: {probe_stats_per_layer}")
    return probe_stats_per_layer

if __name__ == "__main__":
    task = 'mnist_pvr'
    leaky_task = 'pvr_leaky'
    training_args = {
        'batch_size': 512,
        'lr': 0.001,
        'num_workers': 0,
        "epochs": 5
    }
    save_weights = False
    probe_training_args = {
        'batch_size': 1024,
        'lr': 0.001,
        'num_workers': 0,
        'epochs': 10,
    }
    use_wandb = True
    reduction = 'max'
    #####################################
    train_set, test_set = get_dataset(task, dataset_config={})
    ll_model, hl_model, corr = get_alignment(task, config={"input_shape": test_set.get_input_shape()})
    model_pair = IITProbeSequentialPair(ll_model=ll_model, hl_model=hl_model, corr=corr, training_args=training_args) 
    model_pair.train(train_set, train_set, test_set, test_set, 
                     epochs=training_args['epochs'], use_wandb=use_wandb)
    if use_wandb:
        wandb.finish()
    print(f"done training\n------------------------------------")
    print(f"evaluating model")
    leaky_train_set, leaky_test_set = get_dataset(leaky_task, dataset_config={})
    ll_model.eval()
    
    if save_weights:
        if not os.path.exists(f"weights/ll_model"):
            os.makedirs(f"weights/ll_model")
        t.save(ll_model.state_dict(), f"weights/ll_model/{task}.pt")
    print("evaluating on correctness")
    correctness_stats_per_layer = evaluate_model_on_probes(
        ll_model=ll_model, task=task, 
        train_set=train_set, test_set=test_set, 
        probe_training_args=probe_training_args, use_wandb=use_wandb, save_probes=save_weights)
    print("evaluating on leakyness")
    leaky_stats_per_layer = evaluate_model_on_probes(
        ll_model=ll_model, task=leaky_task, 
        train_set=leaky_train_set, test_set=leaky_test_set, 
        probe_training_args=probe_training_args, use_wandb=use_wandb, save_probes=save_weights)
    time = datetime.now().strftime("%d_%m_%Y_%H_%M_%S")
    plot_probe_stats(correctness_stats_per_layer, leaky_stats_per_layer, reduction=reduction, prefix=f'{time}', use_wandb=use_wandb)
    print("done evaluating\n------------------------------------")
